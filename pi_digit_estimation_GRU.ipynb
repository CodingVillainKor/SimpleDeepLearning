{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bDlRyKYTnjE-"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAwmGxvaooYc",
        "outputId": "b203b108-bb6f-4893-a872-c09d46be2e2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000001\n"
          ]
        }
      ],
      "source": [
        "with open(\"pi_million.txt\") as f:\n",
        "    pi = f.read()\n",
        "    pi = pi[0] + pi[2:]\n",
        "print(len(pi)) # \"3.\" + 1,000,000 digits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QyxkcuxnW6Yb"
      },
      "outputs": [],
      "source": [
        "class Dataset:\n",
        "    def __init__(self, pi_str, _start, _end, _len=512):\n",
        "        self.data = self.fetch_data(pi_str, \n",
        "                                    start=_start, \n",
        "                                    end=_end, \n",
        "                                    seq_len=_len)\n",
        "\n",
        "    def fetch_data(self, pi_str, start, end, seq_len):\n",
        "        sequences = [pi_str[i:i+seq_len] for i in range(start, end)]\n",
        "        digit_data = [[int(c) for c in seq] for seq in sequences]\n",
        "\n",
        "        return digit_data\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def col_fn(batch):\n",
        "    return torch.stack([torch.LongTensor(b) for b in batch])\n",
        "\n",
        "train_set = Dataset(pi, _start=0, _end=100000)\n",
        "test_set  = Dataset(pi, _start=100000, _end=103000)\n",
        "train_dl  = DataLoader(train_set, batch_size=256, drop_last=False, collate_fn = col_fn)\n",
        "test_dl   = DataLoader(test_set,  batch_size=256, drop_last=False, collate_fn = col_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JnHQ0BB2oqIO"
      },
      "outputs": [],
      "source": [
        "class Model(nn.Module):\n",
        "    def __init__(self, num_digit, in_dim, hidden_dim, out_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(num_digit, in_dim)\n",
        "        self.in_gru = nn.GRU(in_dim, hidden_dim, batch_first=True)\n",
        "        self.latent_fc = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out_gru = nn.GRU(hidden_dim, out_dim, batch_first=True)\n",
        "        self.out_fc = nn.Linear(out_dim, num_digit)\n",
        "    \n",
        "    def forward(self, x, return_loss=True):\n",
        "        emb_out = self.embedding(x)\n",
        "        hidden, _ = self.in_gru(emb_out)\n",
        "        latent = self.latent_fc(hidden)\n",
        "        out, _ = self.out_gru(latent)\n",
        "        out_digit = self.out_fc(out[:, -2])\n",
        "\n",
        "        if return_loss:\n",
        "            loss = self.get_loss(out_digit, x)\n",
        "            return loss\n",
        "        else:\n",
        "            return out_digit\n",
        "\n",
        "\n",
        "    def get_loss(self, digit_logit, x):\n",
        "        x_last = x[:, -1]\n",
        "\n",
        "        loss = F.cross_entropy(digit_logit, x_last)\n",
        "        return loss\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, x):\n",
        "        digit_logit = self.forward(x, return_loss=False)\n",
        "\n",
        "        prob = torch.softmax(digit_logit, -1)\n",
        "        sample = torch.argmax(prob, -1)\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9515oV3gRJgH"
      },
      "outputs": [],
      "source": [
        "model_config = {\n",
        "    \"num_digit\": 10,\n",
        "    \"in_dim\": 512,\n",
        "    \"hidden_dim\": 1024,\n",
        "    \"out_dim\": 512\n",
        "}\n",
        "\n",
        "model = Model(**model_config)\n",
        "model.to(device)\n",
        "optim = torch.optim.Adam(model.parameters(), lr=0.001, betas=[0.9, 0.999])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQtA2imoulEA",
        "outputId": "cb6eec81-4932-4802-8a8b-c3a213c39e35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "idx: 0 [0 / 391],  loss= 2.306\n",
            "idx: 500 [109 / 391],  loss= 2.308\n",
            "idx: 1,000 [218 / 391],  loss= 2.310\n",
            "idx: 1,500 [327 / 391],  loss= 2.318\n",
            "idx: 2,000 [45 / 391],  loss= 2.309\n",
            "idx: 2,500 [154 / 391],  loss= 2.302\n",
            "idx: 3,000 [263 / 391],  loss= 2.305\n",
            "idx: 3,500 [372 / 391],  loss= 2.310\n",
            "idx: 4,000 [90 / 391],  loss= 2.296\n",
            "idx: 4,500 [199 / 391],  loss= 2.312\n",
            "idx: 5,000 [308 / 391],  loss= 2.313\n",
            "idx: 5,500 [26 / 391],  loss= 2.298\n",
            "idx: 6,000 [135 / 391],  loss= 2.303\n",
            "idx: 6,500 [244 / 391],  loss= 2.315\n",
            "idx: 7,000 [353 / 391],  loss= 2.303\n",
            "idx: 7,500 [71 / 391],  loss= 2.302\n",
            "idx: 8,000 [180 / 391],  loss= 2.304\n",
            "idx: 8,500 [289 / 391],  loss= 2.309\n",
            "idx: 9,000 [7 / 391],  loss= 2.304\n",
            "idx: 9,500 [116 / 391],  loss= 2.300\n",
            "idx: 10,000 [225 / 391],  loss= 2.308\n",
            "idx: 10,500 [334 / 391],  loss= 2.306\n",
            "idx: 11,000 [52 / 391],  loss= 2.309\n",
            "idx: 11,500 [161 / 391],  loss= 2.304\n",
            "idx: 12,000 [270 / 391],  loss= 2.317\n",
            "idx: 12,500 [379 / 391],  loss= 2.308\n",
            "idx: 13,000 [97 / 391],  loss= 2.318\n",
            "idx: 13,500 [206 / 391],  loss= 2.314\n",
            "idx: 14,000 [315 / 391],  loss= 2.300\n",
            "idx: 14,500 [33 / 391],  loss= 2.289\n",
            "idx: 15,000 [142 / 391],  loss= 2.308\n",
            "idx: 15,500 [251 / 391],  loss= 2.318\n",
            "idx: 16,000 [360 / 391],  loss= 2.310\n",
            "idx: 16,500 [78 / 391],  loss= 2.315\n",
            "idx: 17,000 [187 / 391],  loss= 2.304\n",
            "idx: 17,500 [296 / 391],  loss= 2.303\n",
            "idx: 18,000 [14 / 391],  loss= 2.315\n",
            "idx: 18,500 [123 / 391],  loss= 2.302\n",
            "idx: 19,000 [232 / 391],  loss= 2.302\n",
            "idx: 19,500 [341 / 391],  loss= 2.307\n",
            "idx: 19,549 [390 / 391],  loss= 2.316"
          ]
        }
      ],
      "source": [
        "idx = 0\n",
        "epochs = 50\n",
        "for e in range(epochs):\n",
        "    for i, x in enumerate(train_dl):\n",
        "        x = x.to(device)\n",
        "\n",
        "        loss = model(x)\n",
        "        optim.zero_grad()\n",
        "        loss.backward()\n",
        "        optim.step()\n",
        "\n",
        "        print(f\"\\ridx: {idx:,} [{i} / {len(train_dl)}],  loss= {loss.item():.3f}\", end='')\n",
        "        if idx % 500 == 0:\n",
        "            print(\"\")\n",
        "\n",
        "        idx += 1\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"512_1024_10k.ckpt\")"
      ],
      "metadata": {
        "id": "mkqNRrG3LKyX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**\n",
        "Check whether pi estimator works well in validation dataset"
      ],
      "metadata": {
        "id": "3D5PFjHhC8J7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggDal1N6yUdB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d130904a-f279-4c77-aebf-af80d193b066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total_correct = 9999, total_wrong = 1\n"
          ]
        }
      ],
      "source": [
        "def check_correct_wrong(sample, true):\n",
        "    batch_len = len(sample)\n",
        "    correct = sample == true\n",
        "    correct_num = correct.sum()\n",
        "    wrong_num = batch_len - correct_num\n",
        "\n",
        "    return correct_num, wrong_num\n",
        "\n",
        "model_config = {\n",
        "    \"num_digit\": 10,\n",
        "    \"in_dim\": 512,\n",
        "    \"hidden_dim\": 1024,\n",
        "    \"out_dim\": 512\n",
        "}\n",
        "\n",
        "model = Model(**model_config)\n",
        "state_dict = torch.load(\"512_1024_10k.ckpt\")\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)\n",
        "\n",
        "total_correct, total_wrong = 0, 0\n",
        "for x in train_dl:\n",
        "    x = x.to(device)\n",
        "    sample = model.generate(x)\n",
        "    correct_num, wrong_num = check_correct_wrong(sample, x[:, -1])\n",
        "    total_correct += correct_num\n",
        "    total_wrong += wrong_num\n",
        "print(f\"total_correct = {total_correct}, total_wrong = {total_wrong}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3UbnOa4hbv4q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}